{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to play with different Neural Networks to try to model all of the data. I don't seem to have saved a varient of the dataframe where all of the stations are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get list of csv names\n",
    "# with open(\"text_files/files.txt\", \"r\") as f:\n",
    "#     CSVs = f.read()\n",
    "\n",
    "# CSVs = CSVs.replace(\"\\n    \", \" \").replace(\"%20\", \"_\")[2:-2].split(\"', '\")\n",
    "# print(CSVs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug. find which files arent in folder that are expected to be\n",
    "# file_path = f\"data/Journey_Data/\"\n",
    "# downloaded_files = set(os.listdir(file_path))\n",
    "# missing_files = [csv for csv in CSVs if csv not in downloaded_files]\n",
    "# print(missing_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing purposes only select first 50 elements for now (out of 424)\n",
    "# CSVs = CSVs[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 32 files: {'12aJourneyDataExtract15Nov15-27Nov15.csv', '121JourneyDataExtract01Aug2018-07Aug2018.csv', '116JourneyDataExtract27June2018-03July2018.csv', '122JourneyDataExtract08Aug2018-14Aug2018.csv', '109JourneyDataExtract09May2018-15May2018.csv', '11a-Journey-Data-Extract-18Oct15-31Oct15.csv', '129JourneyDataExtract26Sep2018-02Oct2018.csv', '114JourneyDataExtract13June2018-19June2018.csv', '12JourneyDataExtract29Jun2016-05Jul2016.csv', '126JourneyDataExtract05Sep2018-11Sep2018.csv', '10a-Journey-Data-Extract-20Sep15-03Oct15.csv', '10JourneyDataExtract15Jun2016-21Jun2016.csv', '128JourneyDataExtract19Sep2018-25Sep2018.csv', '118JourneyDataExtract11July2018-17July2018.csv', '12bJourneyDataExtract28Nov15-12Dec15.csv', '127JourneyDataExtract12Sep2018-18Sep2018.csv', '112JourneyDataExtract30May2018-05June2018.csv', '117JourneyDataExtract04July2018-10July2018.csv', '124JourneyDataExtract22Aug2018-28Aug2018.csv', '123JourneyDataExtract15Aug2018-21Aug2018.csv', '125JourneyDataExtract29Aug2018-04Sep2018.csv', '120JourneyDataExtract25July2018-31July2018.csv', '10b-Journey-Data-Extract-04Oct15-17Oct15.csv', '119JourneyDataExtract18July2018-24July2018.csv', '108JourneyDataExtract02May2018-08May2018.csv', '11b-Journey-Data-Extract-01Nov15-14Nov15.csv', '130JourneyDataExtract03Oct2018-09Oct2018.csv', '113JourneyDataExtract06June2018-12June2018.csv', '115JourneyDataExtract20June2018-26June2018.csv', '11JourneyDataExtract22Jun2016-28Jun2016.csv', '110JourneyDataExtract16May2018-22May2018.csv', '111JourneyDataExtract23May2018-29May2018.csv'}\n"
     ]
    }
   ],
   "source": [
    "# use CSVs in dir\n",
    "file_path = f\"data/Journey_Data/\"\n",
    "downloaded_files = set(os.listdir(file_path))\n",
    "CSVs = downloaded_files\n",
    "print(f\"There are {len(CSVs)} files: {CSVs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 32/32, merged_df is 647.3640289306641MB large: 100%|██████████| 32/32 [00:03<00:00,  8.92it/s] \n"
     ]
    }
   ],
   "source": [
    "# Specify your S3 bucket and CSV file paths\n",
    "bucket_name = 'tfl-cycle-data'\n",
    "csv_dir = 'Journey_Data/'\n",
    "\n",
    "# Initialize an empty Polars DataFrame\n",
    "merged_df = pl.DataFrame()\n",
    "cols = []\n",
    "\n",
    "progress_bar = tqdm(CSVs, desc=\"Processing\")\n",
    "\n",
    "for idx, csv in enumerate(progress_bar):\n",
    "    file_path = f\"temp/{csv}\"\n",
    "    # Download the CSV file from S3\n",
    "    # s3.download_file(bucket_name, f\"{csv_dir}{csv}\", file_path)\n",
    "    file_path = f\"data/Journey_Data/{csv}\"\n",
    "    df = pl.read_csv(file_path, skip_rows_after_header=1, ignore_errors=True)\n",
    "    # print(df.columns)\n",
    "\n",
    "    # Rename columns and keep only 'Rental ID', 'Start Date', 'Start Station Name', 'End Date', 'End Station Name'\n",
    "    renaming_dict = {\n",
    "        'Number': 'Rental ID',\n",
    "        'rental ID': 'Rental ID',\n",
    "        'Rental Id': 'Rental ID',\n",
    "        'Start date': 'Start Date',\n",
    "        'StartStation Name': 'Start Station Name',\n",
    "        'Start station': 'Start Station Name',\n",
    "        'End date': 'End Date',\n",
    "        'EndStation Name': 'End Station Name',\n",
    "        'End station': 'End Station Name'\n",
    "    }\n",
    "\n",
    "    for old_col, new_col in renaming_dict.items():\n",
    "        if old_col in df.columns:\n",
    "            df = df.rename({old_col: new_col})\n",
    "    \n",
    "    cols_to_keep = ['Rental ID', 'Start Date', 'Start Station Name', 'End Date', 'End Station Name']\n",
    "    df = df.select(cols_to_keep)\n",
    "\n",
    "    # Convert date columns\n",
    "    df = df.with_columns([\n",
    "        pl.col('Start Date').str.strptime(pl.Datetime, strict=False),\n",
    "        pl.col('End Date').str.strptime(pl.Datetime, strict=False),\n",
    "        pl.col('Start Station Name').cast(pl.Utf8),\n",
    "        pl.col('End Station Name').cast(pl.Utf8)\n",
    "    ])\n",
    "\n",
    "    # Filter rows (if needed)\n",
    "    # df = df.filter((pl.col(\"Start Station Name\") == \"Waterloo Station 3, Waterloo\") |\n",
    "    #                (pl.col(\"End Station Name\") == \"Waterloo Station 3, Waterloo\"))\n",
    "    \n",
    "    # Concatenate DataFrames\n",
    "    merged_df = pl.concat([merged_df, df])\n",
    "\n",
    "    progress_bar.set_description(\n",
    "        f\"Processing Index: {idx+1}/{len(CSVs)}, merged_df is {merged_df.estimated_size()/(1024*1024)}MB large\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# save to pickle\n",
    "with open(\"data/df_temp\", 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Rental ID</th><th>Start Date</th><th>Start Station Name</th><th>End Date</th><th>End Station Name</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>str</td><td>datetime[μs]</td><td>str</td></tr></thead><tbody><tr><td>104816169</td><td>2021-01-03 12:57:00</td><td>&quot;New Fetter Lane, Holborn&quot;</td><td>2021-01-03 13:26:00</td><td>&quot;Salmon Lane, Limehouse&quot;</td></tr><tr><td>104757113</td><td>2020-12-30 14:33:00</td><td>&quot;Houndsditch, Aldgate&quot;</td><td>2020-12-30 15:00:00</td><td>&quot;Warren Street Station, Euston&quot;</td></tr><tr><td>104749458</td><td>2020-12-30 09:08:00</td><td>&quot;Simpson Street, Clapham Juncti…</td><td>2020-12-30 09:21:00</td><td>&quot;Ram Street, Wandsworth&quot;</td></tr><tr><td>104788389</td><td>2021-01-01 14:59:00</td><td>&quot;Crabtree Lane, Fulham&quot;</td><td>2021-01-01 16:29:00</td><td>&quot;Crabtree Lane, Fulham&quot;</td></tr><tr><td>104792584</td><td>2021-01-01 20:58:00</td><td>&quot;Crabtree Lane, Fulham&quot;</td><td>2021-01-01 21:02:00</td><td>&quot;Crisp Road, Hammersmith&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌───────────┬──────────────┬───────────────────────┬───────────────────────┬───────────────────────┐\n",
       "│ Rental ID ┆ Start Date   ┆ Start Station Name    ┆ End Date              ┆ End Station Name      │\n",
       "│ ---       ┆ ---          ┆ ---                   ┆ ---                   ┆ ---                   │\n",
       "│ i64       ┆ datetime[μs] ┆ str                   ┆ datetime[μs]          ┆ str                   │\n",
       "╞═══════════╪══════════════╪═══════════════════════╪═══════════════════════╪═══════════════════════╡\n",
       "│ 104816169 ┆ 2021-01-03   ┆ New Fetter Lane,      ┆ 2021-01-03 13:26:00   ┆ Salmon Lane,          │\n",
       "│           ┆ 12:57:00     ┆ Holborn               ┆                       ┆ Limehouse             │\n",
       "│ 104757113 ┆ 2020-12-30   ┆ Houndsditch, Aldgate  ┆ 2020-12-30 15:00:00   ┆ Warren Street         │\n",
       "│           ┆ 14:33:00     ┆                       ┆                       ┆ Station, Euston       │\n",
       "│ 104749458 ┆ 2020-12-30   ┆ Simpson Street,       ┆ 2020-12-30 09:21:00   ┆ Ram Street,           │\n",
       "│           ┆ 09:08:00     ┆ Clapham Juncti…       ┆                       ┆ Wandsworth            │\n",
       "│ 104788389 ┆ 2021-01-01   ┆ Crabtree Lane, Fulham ┆ 2021-01-01 16:29:00   ┆ Crabtree Lane, Fulham │\n",
       "│           ┆ 14:59:00     ┆                       ┆                       ┆                       │\n",
       "│ 104792584 ┆ 2021-01-01   ┆ Crabtree Lane, Fulham ┆ 2021-01-01 21:02:00   ┆ Crisp Road,           │\n",
       "│           ┆ 20:58:00     ┆                       ┆                       ┆ Hammersmith           │\n",
       "└───────────┴──────────────┴───────────────────────┴───────────────────────┴───────────────────────┘"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
