{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to play with different Neural Networks to try to model all of the data. I don't seem to have saved a varient of the dataframe where all of the stations are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23 files: {'06JourneyDataExtract18May2016-24May2016.csv', '100JourneyDataExtract07Mar2018-13Mar2018.csv', '104JourneyDataExtract04Apr2018-10Apr2018.csv', '109JourneyDataExtract09May2018-15May2018.csv', '107JourneyDataExtract25Apr2018-01May2018.csv', '05JourneyDataExtract01May2016-17May2016.csv', '03JourneyDataExtract06Mar2016-31Mar2016.csv', '105JourneyDataExtract11Apr2018-17Apr2018.csv', '07JourneyDataExtract25May2016-31May2016.csv', '10a-Journey-Data-Extract-20Sep15-03Oct15.csv', '10JourneyDataExtract15Jun2016-21Jun2016.csv', '103JourneyDataExtract28Mar2018-03Apr2018.csv', '04JourneyDataExtract01Apr2016-30Apr2016.csv', '01aJourneyDataExtract10Jan16-23Jan16.csv', '02bJourneyDataExtract21Feb16-05Mar2016.csv', '02aJourneyDataExtract07Fe16-20Feb2016.csv', '102JourneyDataExtract21Mar2018-27Mar2018.csv', '101JourneyDataExtract14Mar2018-20Mar2018.csv', '01bJourneyDataExtract24Jan16-06Feb16.csv', '108JourneyDataExtract02May2018-08May2018.csv', '08JourneyDataExtract01Jun2016-07Jun2016.csv', '09JourneyDataExtract08Jun2016-14Jun2016.csv', '106JourneyDataExtract18Apr2018-24Apr2018.csv'}\n"
     ]
    }
   ],
   "source": [
    "# use CSVs in dir\n",
    "# file_path = f\"data/tfl_CSVs/\"\n",
    "directory = f\"data/Journey_Data/\"\n",
    "downloaded_files = set(os.listdir(directory))\n",
    "CSVs = downloaded_files\n",
    "print(f\"There are {len(CSVs)} files: {CSVs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 1/23, merged_df is 16.760796546936035MB large:   4%|▍         | 1/23 [00:00<00:02,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 06JourneyDataExtract18May2016-24May2016.csv\n",
      " file_path: data/Journey_Data/06JourneyDataExtract18May2016-24May2016.csv\n",
      "CSV: 100JourneyDataExtract07Mar2018-13Mar2018.csv\n",
      " file_path: data/Journey_Data/100JourneyDataExtract07Mar2018-13Mar2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 4/23, merged_df is 57.76355171203613MB large:  13%|█▎        | 3/23 [00:00<00:01, 11.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 104JourneyDataExtract04Apr2018-10Apr2018.csv\n",
      " file_path: data/Journey_Data/104JourneyDataExtract04Apr2018-10Apr2018.csv\n",
      "CSV: 109JourneyDataExtract09May2018-15May2018.csv\n",
      " file_path: data/Journey_Data/109JourneyDataExtract09May2018-15May2018.csv\n",
      "CSV: 107JourneyDataExtract25Apr2018-01May2018.csv\n",
      " file_path: data/Journey_Data/107JourneyDataExtract25Apr2018-01May2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 5/23, merged_df is 70.63428020477295MB large:  22%|██▏       | 5/23 [00:00<00:01, 11.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 05JourneyDataExtract01May2016-17May2016.csv\n",
      " file_path: data/Journey_Data/05JourneyDataExtract01May2016-17May2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 7/23, merged_df is 155.6522035598755MB large:  30%|███       | 7/23 [00:00<00:02,  7.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 03JourneyDataExtract06Mar2016-31Mar2016.csv\n",
      " file_path: data/Journey_Data/03JourneyDataExtract06Mar2016-31Mar2016.csv\n",
      "CSV: 105JourneyDataExtract11Apr2018-17Apr2018.csv\n",
      " file_path: data/Journey_Data/105JourneyDataExtract11Apr2018-17Apr2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 9/23, merged_df is 185.81950759887695MB large:  39%|███▉      | 9/23 [00:01<00:01,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 07JourneyDataExtract25May2016-31May2016.csv\n",
      " file_path: data/Journey_Data/07JourneyDataExtract25May2016-31May2016.csv\n",
      "CSV: 10a-Journey-Data-Extract-20Sep15-03Oct15.csv\n",
      " file_path: data/Journey_Data/10a-Journey-Data-Extract-20Sep15-03Oct15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 12/23, merged_df is 242.25210762023926MB large:  48%|████▊     | 11/23 [00:01<00:01,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 10JourneyDataExtract15Jun2016-21Jun2016.csv\n",
      " file_path: data/Journey_Data/10JourneyDataExtract15Jun2016-21Jun2016.csv\n",
      "CSV: 103JourneyDataExtract28Mar2018-03Apr2018.csv\n",
      " file_path: data/Journey_Data/103JourneyDataExtract28Mar2018-03Apr2018.csv\n",
      "CSV: 04JourneyDataExtract01Apr2016-30Apr2016.csv\n",
      " file_path: data/Journey_Data/04JourneyDataExtract01Apr2016-30Apr2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 14/23, merged_df is 321.627534866333MB large:  61%|██████    | 14/23 [00:01<00:01,  7.95it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 01aJourneyDataExtract10Jan16-23Jan16.csv\n",
      " file_path: data/Journey_Data/01aJourneyDataExtract10Jan16-23Jan16.csv\n",
      "CSV: 02bJourneyDataExtract21Feb16-05Mar2016.csv\n",
      " file_path: data/Journey_Data/02bJourneyDataExtract21Feb16-05Mar2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 17/23, merged_df is 376.1432161331177MB large:  70%|██████▉   | 16/23 [00:01<00:00,  8.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 02aJourneyDataExtract07Fe16-20Feb2016.csv\n",
      " file_path: data/Journey_Data/02aJourneyDataExtract07Fe16-20Feb2016.csv\n",
      "CSV: 102JourneyDataExtract21Mar2018-27Mar2018.csv\n",
      " file_path: data/Journey_Data/102JourneyDataExtract21Mar2018-27Mar2018.csv\n",
      "CSV: 101JourneyDataExtract14Mar2018-20Mar2018.csv\n",
      " file_path: data/Journey_Data/101JourneyDataExtract14Mar2018-20Mar2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 19/23, merged_df is 408.36723041534424MB large:  78%|███████▊  | 18/23 [00:02<00:00, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 01bJourneyDataExtract24Jan16-06Feb16.csv\n",
      " file_path: data/Journey_Data/01bJourneyDataExtract24Jan16-06Feb16.csv\n",
      "CSV: 108JourneyDataExtract02May2018-08May2018.csv\n",
      " file_path: data/Journey_Data/108JourneyDataExtract02May2018-08May2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 22/23, merged_df is 461.58660888671875MB large:  96%|█████████▌| 22/23 [00:02<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 08JourneyDataExtract01Jun2016-07Jun2016.csv\n",
      " file_path: data/Journey_Data/08JourneyDataExtract01Jun2016-07Jun2016.csv\n",
      "CSV: 09JourneyDataExtract08Jun2016-14Jun2016.csv\n",
      " file_path: data/Journey_Data/09JourneyDataExtract08Jun2016-14Jun2016.csv\n",
      "CSV: 106JourneyDataExtract18Apr2018-24Apr2018.csv\n",
      " file_path: data/Journey_Data/106JourneyDataExtract18Apr2018-24Apr2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 23/23, merged_df is 482.20048904418945MB large: 100%|██████████| 23/23 [00:02<00:00,  9.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty Polars DataFrame\n",
    "merged_df = pl.DataFrame()\n",
    "cols = []\n",
    "\n",
    "progress_bar = tqdm(CSVs, desc=\"Processing\")\n",
    "\n",
    "for idx, csv in enumerate(progress_bar):\n",
    "    file_path = f\"{directory}{csv}\"\n",
    "    print(f\"CSV: {csv}\\n file_path: {file_path}\")\n",
    "    df = pl.read_csv(file_path, skip_rows_after_header=1, ignore_errors=True)\n",
    "    # print(df.columns)\n",
    "\n",
    "    # Rename columns and keep only 'Rental ID', 'Start Date', 'Start Station Name', 'End Date', 'End Station Name'\n",
    "    renaming_dict = {\n",
    "        'Number': 'Rental ID',\n",
    "        'rental ID': 'Rental ID',\n",
    "        'Rental Id': 'Rental ID',\n",
    "        'Start date': 'Start Date',\n",
    "        'StartStation Name': 'Start Station Name',\n",
    "        'Start station': 'Start Station Name',\n",
    "        'End date': 'End Date',\n",
    "        'EndStation Name': 'End Station Name',\n",
    "        'End station': 'End Station Name'\n",
    "    }\n",
    "\n",
    "    for old_col, new_col in renaming_dict.items():\n",
    "        if old_col in df.columns:\n",
    "            df = df.rename({old_col: new_col})\n",
    "    \n",
    "    cols_to_keep = ['Rental ID', 'Start Date', 'Start Station Name', 'End Date', 'End Station Name']\n",
    "    df = df.select(cols_to_keep)\n",
    "\n",
    "    # Convert date columns\n",
    "    df = df.with_columns([\n",
    "        pl.col('Start Date').str.strptime(pl.Datetime, strict=False),\n",
    "        pl.col('End Date').str.strptime(pl.Datetime, strict=False),\n",
    "        pl.col('Start Station Name').cast(pl.Utf8),\n",
    "        pl.col('End Station Name').cast(pl.Utf8)\n",
    "    ])\n",
    "\n",
    "    # Filter rows (if needed)\n",
    "    # df = df.filter((pl.col(\"Start Station Name\") == \"Waterloo Station 3, Waterloo\") |\n",
    "    #                (pl.col(\"End Station Name\") == \"Waterloo Station 3, Waterloo\"))\n",
    "    \n",
    "    # Concatenate DataFrames\n",
    "    merged_df = pl.concat([merged_df, df])\n",
    "\n",
    "    progress_bar.set_description(\n",
    "        f\"Processing Index: {idx+1}/{len(CSVs)}, merged_df is {merged_df.estimated_size()/(1024*1024)}MB large\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Rental ID</th><th>Start Date</th><th>Start Station Name</th><th>End Date</th><th>End Station Name</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>str</td><td>datetime[μs]</td><td>str</td></tr></thead><tbody><tr><td>74656682</td><td>2018-04-18 07:32:00</td><td>&quot;Crosswall, Tower&quot;</td><td>2018-04-18 07:43:00</td><td>&quot;Tallis Street, Temple&quot;</td></tr><tr><td>74699122</td><td>2018-04-19 07:31:00</td><td>&quot;Crosswall, Tower&quot;</td><td>2018-04-19 07:42:00</td><td>&quot;Tallis Street, Temple&quot;</td></tr><tr><td>74831922</td><td>2018-04-21 22:42:00</td><td>&quot;Moor Street, Soho&quot;</td><td>2018-04-21 23:14:00</td><td>&quot;Geraldine Street, Elephant &amp; C…</td></tr><tr><td>74740907</td><td>2018-04-19 22:51:00</td><td>&quot;Moor Street, Soho&quot;</td><td>2018-04-19 23:10:00</td><td>&quot;Walworth Road, Elephant &amp; Cast…</td></tr><tr><td>74787286</td><td>2018-04-20 22:31:00</td><td>&quot;Moor Street, Soho&quot;</td><td>2018-04-20 22:46:00</td><td>&quot;Ontario Street, Elephant &amp; Cas…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌───────────┬─────────────────────┬────────────────────┬─────────────────────┬─────────────────────┐\n",
       "│ Rental ID ┆ Start Date          ┆ Start Station Name ┆ End Date            ┆ End Station Name    │\n",
       "│ ---       ┆ ---                 ┆ ---                ┆ ---                 ┆ ---                 │\n",
       "│ i64       ┆ datetime[μs]        ┆ str                ┆ datetime[μs]        ┆ str                 │\n",
       "╞═══════════╪═════════════════════╪════════════════════╪═════════════════════╪═════════════════════╡\n",
       "│ 74656682  ┆ 2018-04-18 07:32:00 ┆ Crosswall, Tower   ┆ 2018-04-18 07:43:00 ┆ Tallis Street,      │\n",
       "│           ┆                     ┆                    ┆                     ┆ Temple              │\n",
       "│ 74699122  ┆ 2018-04-19 07:31:00 ┆ Crosswall, Tower   ┆ 2018-04-19 07:42:00 ┆ Tallis Street,      │\n",
       "│           ┆                     ┆                    ┆                     ┆ Temple              │\n",
       "│ 74831922  ┆ 2018-04-21 22:42:00 ┆ Moor Street, Soho  ┆ 2018-04-21 23:14:00 ┆ Geraldine Street,   │\n",
       "│           ┆                     ┆                    ┆                     ┆ Elephant & C…       │\n",
       "│ 74740907  ┆ 2018-04-19 22:51:00 ┆ Moor Street, Soho  ┆ 2018-04-19 23:10:00 ┆ Walworth Road,      │\n",
       "│           ┆                     ┆                    ┆                     ┆ Elephant & Cast…    │\n",
       "│ 74787286  ┆ 2018-04-20 22:31:00 ┆ Moor Street, Soho  ┆ 2018-04-20 22:46:00 ┆ Ontario Street,     │\n",
       "│           ┆                     ┆                    ┆                     ┆ Elephant & Cas…     │\n",
       "└───────────┴─────────────────────┴────────────────────┴─────────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_dir = \"pickle/df_temp\"\n",
    "\n",
    "# # save to pickle\n",
    "# with open(pickle_dir, 'wb') as f:\n",
    "#     pickle.dump(df, f)\n",
    "\n",
    "#load from pickle\n",
    "with open(pickle_dir, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (786, 2)\n",
      "┌─────┬─────────────────────────────────┐\n",
      "│ ID  ┆ Station Name                    │\n",
      "│ --- ┆ ---                             │\n",
      "│ u32 ┆ str                             │\n",
      "╞═════╪═════════════════════════════════╡\n",
      "│ 0   ┆ Abbey Orchard Street, Westmins… │\n",
      "│ 1   ┆ Abbotsbury Road, Holland Park   │\n",
      "│ 2   ┆ Aberdeen Place, St. John's Woo… │\n",
      "│ 3   ┆ Aberfeldy Street, Poplar        │\n",
      "│ 4   ┆ Abingdon Green, Westminster     │\n",
      "│ …   ┆ …                               │\n",
      "│ 781 ┆ Wormwood Street, Liverpool Str… │\n",
      "│ 782 ┆ Wren Street, Holborn            │\n",
      "│ 783 ┆ Wright's Lane, Kensington       │\n",
      "│ 784 ┆ Wynne Road, Stockwell           │\n",
      "│ 785 ┆ York Hall, Bethnal Green        │\n",
      "└─────┴─────────────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41762/363409236.py:9: DeprecationWarning: `DataFrame.with_row_count` is deprecated. Use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  id_mapping = unique_values.with_row_count(\"ID\")\n"
     ]
    }
   ],
   "source": [
    "start_stations = df.select(pl.col('Start Station Name').alias('Station Name'))\n",
    "end_stations = df.select(pl.col('End Station Name').alias('Station Name'))\n",
    "\n",
    "unique_values = (\n",
    "    pl.concat([start_stations, end_stations])\n",
    "    .unique()\n",
    "    .sort(\"Station Name\")\n",
    ")\n",
    "id_mapping = unique_values.with_row_count(\"ID\")\n",
    "print(id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Rental ID</th><th>Start Date</th><th>Start Station Name</th><th>End Date</th><th>End Station Name</th><th>Start_ID</th><th>End_ID</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>str</td><td>datetime[μs]</td><td>str</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>74656682</td><td>2018-04-18 07:32:00</td><td>&quot;Crosswall, Tower&quot;</td><td>2018-04-18 07:43:00</td><td>&quot;Tallis Street, Temple&quot;</td><td>182</td><td>684</td></tr><tr><td>74699122</td><td>2018-04-19 07:31:00</td><td>&quot;Crosswall, Tower&quot;</td><td>2018-04-19 07:42:00</td><td>&quot;Tallis Street, Temple&quot;</td><td>182</td><td>684</td></tr><tr><td>74831922</td><td>2018-04-21 22:42:00</td><td>&quot;Moor Street, Soho&quot;</td><td>2018-04-21 23:14:00</td><td>&quot;Geraldine Street, Elephant &amp; C…</td><td>452</td><td>268</td></tr><tr><td>74740907</td><td>2018-04-19 22:51:00</td><td>&quot;Moor Street, Soho&quot;</td><td>2018-04-19 23:10:00</td><td>&quot;Walworth Road, Elephant &amp; Cast…</td><td>452</td><td>731</td></tr><tr><td>74787286</td><td>2018-04-20 22:31:00</td><td>&quot;Moor Street, Soho&quot;</td><td>2018-04-20 22:46:00</td><td>&quot;Ontario Street, Elephant &amp; Cas…</td><td>452</td><td>496</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌───────────┬────────────────┬────────────────┬────────────────┬───────────────┬──────────┬────────┐\n",
       "│ Rental ID ┆ Start Date     ┆ Start Station  ┆ End Date       ┆ End Station   ┆ Start_ID ┆ End_ID │\n",
       "│ ---       ┆ ---            ┆ Name           ┆ ---            ┆ Name          ┆ ---      ┆ ---    │\n",
       "│ i64       ┆ datetime[μs]   ┆ ---            ┆ datetime[μs]   ┆ ---           ┆ u32      ┆ u32    │\n",
       "│           ┆                ┆ str            ┆                ┆ str           ┆          ┆        │\n",
       "╞═══════════╪════════════════╪════════════════╪════════════════╪═══════════════╪══════════╪════════╡\n",
       "│ 74656682  ┆ 2018-04-18     ┆ Crosswall,     ┆ 2018-04-18     ┆ Tallis        ┆ 182      ┆ 684    │\n",
       "│           ┆ 07:32:00       ┆ Tower          ┆ 07:43:00       ┆ Street,       ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Temple        ┆          ┆        │\n",
       "│ 74699122  ┆ 2018-04-19     ┆ Crosswall,     ┆ 2018-04-19     ┆ Tallis        ┆ 182      ┆ 684    │\n",
       "│           ┆ 07:31:00       ┆ Tower          ┆ 07:42:00       ┆ Street,       ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Temple        ┆          ┆        │\n",
       "│ 74831922  ┆ 2018-04-21     ┆ Moor Street,   ┆ 2018-04-21     ┆ Geraldine     ┆ 452      ┆ 268    │\n",
       "│           ┆ 22:42:00       ┆ Soho           ┆ 23:14:00       ┆ Street,       ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Elephant & C… ┆          ┆        │\n",
       "│ 74740907  ┆ 2018-04-19     ┆ Moor Street,   ┆ 2018-04-19     ┆ Walworth      ┆ 452      ┆ 731    │\n",
       "│           ┆ 22:51:00       ┆ Soho           ┆ 23:10:00       ┆ Road,         ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Elephant &    ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Cast…         ┆          ┆        │\n",
       "│ 74787286  ┆ 2018-04-20     ┆ Moor Street,   ┆ 2018-04-20     ┆ Ontario       ┆ 452      ┆ 496    │\n",
       "│           ┆ 22:31:00       ┆ Soho           ┆ 22:46:00       ┆ Street,       ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Elephant &    ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Cas…          ┆          ┆        │\n",
       "└───────────┴────────────────┴────────────────┴────────────────┴───────────────┴──────────┴────────┘"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_ids = df.join(id_mapping.rename({\"Station Name\": \"Start Station Name\", \"ID\": \"Start_ID\"}), on=\"Start Station Name\", how=\"left\")\n",
    "df_with_ids = df_with_ids.join(id_mapping.rename({\"Station Name\": \"End Station Name\", \"ID\": \"End_ID\"}), on=\"End Station Name\", how=\"left\")\n",
    "\n",
    "df_with_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Rental ID</th><th>Start Date</th><th>End Date</th><th>Start_ID</th><th>End_ID</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>datetime[μs]</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>74656682</td><td>2018-04-18 07:32:00</td><td>2018-04-18 07:43:00</td><td>182</td><td>684</td></tr><tr><td>74699122</td><td>2018-04-19 07:31:00</td><td>2018-04-19 07:42:00</td><td>182</td><td>684</td></tr><tr><td>74831922</td><td>2018-04-21 22:42:00</td><td>2018-04-21 23:14:00</td><td>452</td><td>268</td></tr><tr><td>74740907</td><td>2018-04-19 22:51:00</td><td>2018-04-19 23:10:00</td><td>452</td><td>731</td></tr><tr><td>74787286</td><td>2018-04-20 22:31:00</td><td>2018-04-20 22:46:00</td><td>452</td><td>496</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌───────────┬─────────────────────┬─────────────────────┬──────────┬────────┐\n",
       "│ Rental ID ┆ Start Date          ┆ End Date            ┆ Start_ID ┆ End_ID │\n",
       "│ ---       ┆ ---                 ┆ ---                 ┆ ---      ┆ ---    │\n",
       "│ i64       ┆ datetime[μs]        ┆ datetime[μs]        ┆ u32      ┆ u32    │\n",
       "╞═══════════╪═════════════════════╪═════════════════════╪══════════╪════════╡\n",
       "│ 74656682  ┆ 2018-04-18 07:32:00 ┆ 2018-04-18 07:43:00 ┆ 182      ┆ 684    │\n",
       "│ 74699122  ┆ 2018-04-19 07:31:00 ┆ 2018-04-19 07:42:00 ┆ 182      ┆ 684    │\n",
       "│ 74831922  ┆ 2018-04-21 22:42:00 ┆ 2018-04-21 23:14:00 ┆ 452      ┆ 268    │\n",
       "│ 74740907  ┆ 2018-04-19 22:51:00 ┆ 2018-04-19 23:10:00 ┆ 452      ┆ 731    │\n",
       "│ 74787286  ┆ 2018-04-20 22:31:00 ┆ 2018-04-20 22:46:00 ┆ 452      ┆ 496    │\n",
       "└───────────┴─────────────────────┴─────────────────────┴──────────┴────────┘"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brief = df_with_ids[\"Rental ID\", \"Start Date\", \"End Date\", \"Start_ID\", \"End_ID\"]\n",
    "df_brief.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (50, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Rental ID</th><th>Date</th><th>ID</th><th>flow</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>u32</td><td>i32</td></tr></thead><tbody><tr><td>74654037</td><td>2018-04-18 00:00:00</td><td>551</td><td>-1</td></tr><tr><td>74654041</td><td>2018-04-18 00:00:00</td><td>204</td><td>-1</td></tr><tr><td>74654038</td><td>2018-04-18 00:00:00</td><td>552</td><td>-1</td></tr><tr><td>74654036</td><td>2018-04-18 00:00:00</td><td>670</td><td>-1</td></tr><tr><td>74654039</td><td>2018-04-18 00:00:00</td><td>695</td><td>-1</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>74654057</td><td>2018-04-18 00:10:00</td><td>587</td><td>1</td></tr><tr><td>74654066</td><td>2018-04-18 00:10:00</td><td>361</td><td>1</td></tr><tr><td>74654049</td><td>2018-04-18 00:10:00</td><td>83</td><td>1</td></tr><tr><td>74654075</td><td>2018-04-18 00:11:00</td><td>540</td><td>-1</td></tr><tr><td>74654076</td><td>2018-04-18 00:11:00</td><td>556</td><td>-1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (50, 4)\n",
       "┌───────────┬─────────────────────┬─────┬──────┐\n",
       "│ Rental ID ┆ Date                ┆ ID  ┆ flow │\n",
       "│ ---       ┆ ---                 ┆ --- ┆ ---  │\n",
       "│ i64       ┆ datetime[μs]        ┆ u32 ┆ i32  │\n",
       "╞═══════════╪═════════════════════╪═════╪══════╡\n",
       "│ 74654037  ┆ 2018-04-18 00:00:00 ┆ 551 ┆ -1   │\n",
       "│ 74654041  ┆ 2018-04-18 00:00:00 ┆ 204 ┆ -1   │\n",
       "│ 74654038  ┆ 2018-04-18 00:00:00 ┆ 552 ┆ -1   │\n",
       "│ 74654036  ┆ 2018-04-18 00:00:00 ┆ 670 ┆ -1   │\n",
       "│ 74654039  ┆ 2018-04-18 00:00:00 ┆ 695 ┆ -1   │\n",
       "│ …         ┆ …                   ┆ …   ┆ …    │\n",
       "│ 74654057  ┆ 2018-04-18 00:10:00 ┆ 587 ┆ 1    │\n",
       "│ 74654066  ┆ 2018-04-18 00:10:00 ┆ 361 ┆ 1    │\n",
       "│ 74654049  ┆ 2018-04-18 00:10:00 ┆ 83  ┆ 1    │\n",
       "│ 74654075  ┆ 2018-04-18 00:11:00 ┆ 540 ┆ -1   │\n",
       "│ 74654076  ┆ 2018-04-18 00:11:00 ┆ 556 ┆ -1   │\n",
       "└───────────┴─────────────────────┴─────┴──────┘"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_start = df_brief[\"Rental ID\", \"Start Date\", \"Start_ID\"].rename({\"Start Date\": \"Date\", \"Start_ID\": \"ID\"}).with_columns(pl.lit(-1).alias(\"flow\"))\n",
    "\n",
    "df_end = df_brief[\"Rental ID\", \"End Date\", \"End_ID\"].rename({\"End Date\": \"Date\", \"End_ID\": \"ID\"}).with_columns(pl.lit(1).alias(\"flow\"))\n",
    "\n",
    "df_flows = pl.concat([df_start, df_end]).sort(\"Date\")\n",
    "df_flows.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
