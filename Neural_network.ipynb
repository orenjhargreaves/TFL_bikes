{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to play with different Neural Networks to try to model all of the data. I don't seem to have saved a varient of the dataframe where all of the stations are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 417 files: {'229JourneyDataExtract26Aug2020-01Sep2020.csv', '344JourneyDataExtract14Nov2022-20Nov2022.csv', '316JourneyDataExtract04May2022-10May2022.csv', '308JourneyDataExtract09Mar2022-15Mar2022.csv', '317JourneyDataExtract11May2022-17May2022.csv', '65JourneyDataExtract05Jul2017-11Jul2017.csv', '333JourneyDataExtract31Aug2022-06Sep2022.csv', '120JourneyDataExtract25July2018-31July2018.csv', '104JourneyDataExtract04Apr2018-10Apr2018.csv', '91JourneyDataExtract03Jan2018-09Jan2018.csv', '145JourneyDataExtract16Jan2019-22Jan2019.csv', '26JourneyDataExtract05Oct2016-11Oct2016.csv', '94JourneyDataExtract24Jan2018-30Jan2018.csv', '108JourneyDataExtract02May2018-08May2018.csv', '04JourneyDataExtract01Apr2016-30Apr2016.csv', '268JourneyDataExtract02Jun2021-08Jun2021.csv', '16JourneyDataExtract27Jul2016-02Aug2016.csv', '345JourneyDataExtract21Nov2022-27Nov2022.csv', '197JourneyDataExtract15Jan2020-21Jan2020.csv', '306JourneyDataExtract23Feb2022-01Mar2022.csv', '346JourneyDataExtract28Nov2022-04Dec2022.csv', '295JourneyDataExtract08Dec2021-14Dec2021.csv', '143JourneyDataExtract02Jan2019-08Jan2019.csv', '275JourneyDataExtract21Jul2021-27Jul2021.csv', '12aJourneyDataExtract15Nov15-27Nov15.csv', '86JourneyDataExtract29Nov2017-05Dec2017.csv', '320JourneyDataExtract01Jun2022-07Jun2022.csv', '286JourneyDataExtract06Oct2021-12Oct2021.csv', '329JourneyDataExtract03Aug2022-09Aug2022.csv', '282JourneyDataExtract08Sep2021-14Sep2021.csv', '105JourneyDataExtract11Apr2018-17Apr2018.csv', '323JourneyDataExtract22Jun2022-28Jun2022.csv', '186JourneyDataExtract30Oct2019-05Nov2019.csv', '297JourneyDataExtract22Dec2021-28Dec2021.csv', '02bJourneyDataExtract21Feb16-05Mar2016.csv', '2b.JourneyDataExtract15Feb15-28Feb15.csv', '356JourneyDataExtract06Feb2023-12Feb2023.csv', '220JourneyDataExtract24Jun2020-30Jun2020.csv', '302JourneyDataExtract26Jan2022-01Feb2022.csv', '277JourneyDataExtract04Aug2021-10Aug2021.csv', '6aJourneyDataExtract31May15-12Jun15.csv', '160JourneyDataExtract01May2019-07May2019.csv', '48JourneyDataExtract08Mar2017-14Mar2017.csv', '116JourneyDataExtract27June2018-03July2018.csv', '163JourneyDataExtract22May2019-28May2019.csv', '351JourneyDataExtract02Jan2023-08Jan2023.csv', '245JourneyDataExtract16Dec2020-22Dec2020.csv', '75JourneyDataExtract13Sep2017-19Sep2017.csv', '62JourneyDataExtract14Jun2017-20Jun2017.csv', '184JourneyDataExtract16Oct2019-22Oct2019.csv', '27JourneyDataExtract12Oct2016-18Oct2016.csv', '250JourneyDataExtract27Jan2021-02Feb2021.csv', '249JourneyDataExtract20Jan2021-26Jan2021.csv', '135JourneyDataExtract07Nov2018-13Nov2018.csv', '7b.JourneyDataExtract12Jul15-25Jul15.csv', '340JourneyDataExtract17Oct2022-23Oct2022.csv', '255JourneyDataExtract03Mar2021-09Mar2021.csv', '218JourneyDataExtract10Jun2020-16Jun2020.csv', '159JourneyDataExtract24Apr2019-30Apr2019.csv', '338JourneyDataExtract03Oct2022-09Oct2022.csv', '106JourneyDataExtract18Apr2018-24Apr2018.csv', '61JourneyDataExtract07Jun2017-13Jun2017.csv', '222JourneyDataExtract08Jul2020-14Jul2020.csv', '85JourneyDataExtract22Nov2017-28Nov2017.csv', '70JourneyDataExtract08Aug2017-14Aug2017.csv', '115JourneyDataExtract20June2018-26June2018.csv', '332JourneyDataExtract24Aug2022-30Aug2022.csv', '361JourneyDataExtract13Mar2023-19Mar2023.csv', '66JourneyDataExtract12Jul2017-18Jul2017.csv', '5b.JourneyDataExtract17May15-30May15.csv', '10b-Journey-Data-Extract-04Oct15-17Oct15.csv', '140JourneyDataExtract12Dec2018-18Dec2018.csv', '90JourneyDataExtract27Dec2017-02Jan2018.csv', '198JourneyDataExtract22Jan2020-28Jan2020.csv', '33JourneyDataExtract23Nov2016-29Nov2016.csv', '285JourneyDataExtract29Sep2021-05Oct2021.csv', '273JourneyDataExtract07Jul2021-13Jul2021.csv', '21JourneyDataExtract31Aug2016-06Sep2016.csv', '215JourneyDataExtract20May2020-26May2020.csv', '20JourneyDataExtract24Aug2016-30Aug2016.csv', '22JourneyDataExtract07Sep2016-13Sep2016.csv', '265JourneyDataExtract12May2021-18May2021.csv', '321JourneyDataExtract08Jun2022-14Jun2022.csv', '60JourneyDataExtract31May2017-06Jun2017.csv', '238JourneyDataExtract28Oct2020-03Nov2020.csv', '319JourneyDataExtract25May2022-31May2022.csv', '201JourneyDataExtract12Feb2020-18Feb2020.csv', '169JourneyDataExtract03Jul2019-09Jul2019.csv', '221JourneyDataExtract01Jul2020-07Jul2020.csv', '07JourneyDataExtract25May2016-31May2016.csv', '119JourneyDataExtract18July2018-24July2018.csv', '1a.JourneyDataExtract04Jan15-17Jan15.csv', '74JourneyDataExtract06Sep2017-12Sep2017.csv', '387JourneyDataExtract01Jan2024-14Jan2024.csv', '209JourneyDataExtract08Apr2020-14Apr2020.csv', '32JourneyDataExtract16Nov2016-22Nov2016.csv', '287JourneyDataExtract13Oct2021-19Oct2021.csv', '207JourneyDataExtract25Mar2020-31Mar2020.csv', '276JourneyDataExtract28Jul2021-03Aug2021.csv', '53JourneyDataExtract12Apr2017-18Apr2017.csv', '178JourneyDataExtract04Sep2019-10Sep2019.csv', '305JourneyDataExtract16Feb2022-22Feb2022.csv', '251JourneyDataExtract03Feb2021-09Feb2021.csv', '296JourneyDataExtract15Dec2021-21Dec2021.csv', '134JourneyDataExtract31Oct2018-06Nov2018.csv', '315JourneyDataExtract27Apr2022-03May2022.csv', '262JourneyDataExtract21Apr2021-27Apr2021.csv', '154JourneyDataExtract20Mar2019-26Mar2019.csv', '6bJourneyDataExtract13Jun15-27Jun15.csv', '369JourneyDataExtract08May2023-14May2023.csv', '370JourneyDataExtract15May2023-21May2023.csv', '8a-Journey-Data-Extract-26Jul15-07Aug15.csv', '125JourneyDataExtract29Aug2018-04Sep2018.csv', '194JourneyDataExtract25Dec2019-31Dec2019.csv', '3a.JourneyDataExtract01Mar15-15Mar15.csv', '307JourneyDataExtract02Mar2022-08Mar2022.csv', '261JourneyDataExtract14Apr2021-20Apr2021.csv', '373JourneyDataExtract05Jun2023-11Jun2023.csv', '196JourneyDataExtract08Jan2020-14Jan2020.csv', '239JourneyDataExtract04Nov2020-10Nov2020.csv', '223JourneyDataExtract15Jul2020-21Jul2020.csv', '31JourneyDataExtract09Nov2016-15Nov2016.csv', '227JourneyDataExtract12Aug2020-18Aug2020.csv', '214JourneyDataExtract13May2020-19May2020.csv', '05JourneyDataExtract01May2016-17May2016.csv', '8b-Journey-Data-Extract-08Aug15-22Aug15.csv', '335JourneyDataExtract12Sep2022-18Sep2022.csv', '133JourneyDataExtract24Oct2018-30Oct2018.csv', '300JourneyDataExtract12Jan2022-18Jan2022.csv', '348JourneyDataExtract12Dec2022-18Dec2022.csv', '301JourneyDataExtract19Jan2022-25Jan2022.csv', '365JourneyDataExtract10Apr2023-16Apr2023.csv', '190JourneyDataExtract27Nov2019-03Dec2019.csv', '193JourneyDataExtract18Dec2019-24Dec2019.csv', '288JourneyDataExtract20Oct2021-26Oct2021.csv', '08JourneyDataExtract01Jun2016-07Jun2016.csv', '175JourneyDataExtract14Aug2019-20Aug2019.csv', '192JourneyDataExtract11Dec2019-17Dec2019.csv', '248JourneyDataExtract13Jan2021-19Jan2021.csv', '379JourneyDataExtract01Sep2023-14Sep2023.csv', '383JourneyDataExtract01Nov2023-14Nov2023.csv', '334JourneyDataExtract07Sep2022-11Sep2022.csv', '304JourneyDataExtract09Feb2022-15Feb2022.csv', '158JourneyDataExtract17Apr2019-23Apr2019.csv', '146JourneyDataExtract23Jan2019-29Jan2019.csv', '360JourneyDataExtract06Mar2023-12Mar2023.csv', '303JourneyDataExtract02Feb2022-08Feb2022.csv', '377JourneyDataExtract15Jul2023-31Jul2023.csv', '324JourneyDataExtract29Jun2022-05Jul2022.csv', '42JourneyDataExtract25Jan2017-31Jan2017.csv', '258JourneyDataExtract24Mar2021-30Mar2021.csv', '279JourneyDataExtract18Aug2021-24Aug2021.csv', '126JourneyDataExtract05Sep2018-11Sep2018.csv', '199JourneyDataExtract29Jan2020-04Feb2020.csv', '263JourneyDataExtract28Apr2021-04May2021.csv', '41JourneyDataExtract18Jan2017-24Jan2017.csv', '18JourneyDataExtract10Aug2016-16Aug2016.csv', '147JourneyDataExtract30Jan2019-05Feb2019.csv', '388JourneyDataExtract15Jan2024-31Jan2024.csv', '9b-Journey-Data-Extract-06Sep15-19Sep15.csv', '25JourneyDataExtract28Sep2016-04Oct2016.csv', '44JourneyDataExtract08Feb2017-14Feb2017.csv', '14JourneyDataExtract13Jul2016-19Jul2016.csv', '35JourneyDataExtract07Dec2016-13Dec2016.csv', '113JourneyDataExtract06June2018-12June2018.csv', '06JourneyDataExtract18May2016-24May2016.csv', '237JourneyDataExtract21Oct2020-27Oct2020.csv', '09JourneyDataExtract08Jun2016-14Jun2016.csv', '34JourneyDataExtract30Nov2016-06Dec2016.csv', '141JourneyDataExtract19Dec2018-25Dec2018.csv', '121JourneyDataExtract01Aug2018-07Aug2018.csv', '294JourneyDataExtract01Dec2021-07Dec2021.csv', '289JourneyDataExtract27Oct2021-02Nov2021.csv', '217JourneyDataExtract03Jun2020-09Jun2020.csv', '129JourneyDataExtract26Sep2018-02Oct2018.csv', '230JourneyDataExtract02Sep2020-08Sep2020.csv', '313JourneyDataExtract13Apr2022-19Apr2022.csv', '330JourneyDataExtract10Aug2022-16Aug2022.csv', '233JourneyDataExtract23Sep2020-29Sep2020.csv', '2a.JourneyDataExtract01Feb15-14Feb15.csv', '200JourneyDataExtract05Feb2020-11Feb2020.csv', '354JourneyDataExtract23Jan2023-29Jan2023.csv', '309JourneyDataExtract16Mar2022-22Mar2022.csv', '342JourneyDataExtract31Oct2022-06Nov2022.csv', '353JourneyDataExtract16Jan2023-22Jan2023.csv', '358JourneyDataExtract20Feb2023-26Feb2023.csv', '68JourneyDataExtract26Jul2017-31Jul2017.csv', '144JourneyDataExtract09Jan2019-15Jan2019.csv', '82JourneyDataExtract01Nov2017-07Nov2017.csv', '191JourneyDataExtract04Dec2019-10Dec2019.csv', '1b.JourneyDataExtract18Jan15-31Jan15.csv', '176JourneyDataExtract21Aug2019-27Aug2019.csv', '39JourneyDataExtract04Jan2017-10Jan2017.csv', '149JourneyDataExtract13Feb2019-19Feb2019.csv', '278JourneyDataExtract11Aug2021-17Aug2021.csv', '210JourneyDataExtract15Apr2020-21Apr2020.csv', '73JourneyDataExtract30Aug2017-05Sep2017.csv', '37JourneyDataExtract21Dec2016-27Dec2016.csv', '312JourneyDataExtract06Apr2022-12Apr2022.csv', '181JourneyDataExtract25Sep2019-01Oct2019.csv', '12JourneyDataExtract29Jun2016-05Jul2016.csv', '111JourneyDataExtract23May2018-29May2018.csv', '352JourneyDataExtract09Jan2023-15Jan2023.csv', '7a.JourneyDataExtract28Jun15-11Jul15.csv', '391JourneyDataExtract01Mar2024-14Mar2024.csv', '93JourneyDataExtract17Jan2018-23Jan2018.csv', '138JourneyDataExtract28Nov2018-04Dec2018.csv', '252JourneyDataExtract10Feb2021-16Feb2021.csv', '19JourneyDataExtract17Aug2016-23Aug2016.csv', '376JourneyDataExtract01Jul2023-14Jul2023.csv', '173JourneyDataExtract31Jul2019-06Aug2019.csv', '280JourneyDataExtract25Aug2021-31Aug2021.csv', '240JourneyDataExtract11Nov2020-17Nov2020.csv', '162JourneyDataExtract15May2019-21May2019.csv', '10JourneyDataExtract15Jun2016-21Jun2016.csv', '179JourneyDataExtract11Sep2019-17Sep2019.csv', '389JourneyDataExtract01Feb2024-14Feb2024.csv', '30JourneyDataExtract02Nov2016-08Nov2016.csv', '331JourneyDataExtract17Aug2022-23Aug2022.csv', '299JourneyDataExtract05Jan2022-11Jan2022.csv', '253JourneyDataExtract17Feb2021-23Feb2021.csv', '384JourneyDataExtract15Nov2023-30Nov2023.csv', '381JourneyDataExtract01Oct2023-14Oct2023.csv', '216JourneyDataExtract27May2020-02Jun2020.csv', '219JourneyDataExtract17Jun2020-23Jun2020.csv', '88JourneyDataExtract13Dec2017-19Dec2017.csv', '349JourneyDataExtract19Dec2022-25Dec2022.csv', '78JourneyDataExtract04Oct2017-10Oct2017.csv', '122JourneyDataExtract08Aug2018-14Aug2018.csv', '205JourneyDataExtract11Mar2020-17Mar2020.csv', '264JourneyDataExtract05May2021-11May2021.csv', '164JourneyDataExtract29May2019-04Jun2019.csv', '211JourneyDataExtract22Apr2020-28Apr2020.csv', '155JourneyDataExtract27Mar2019-02Apr2019.csv', '293JourneyDataExtract24Nov2021-30Nov2021.csv', '350JourneyDataExtract26Dec2022-01Jan2023.csv', '341JourneyDataExtract24Oct2022-30Oct2022.csv', '03JourneyDataExtract06Mar2016-31Mar2016.csv', '180JourneyDataExtract18Sep2019-24Sep2019.csv', '131JourneyDataExtract10Oct2018-16Oct2018.csv', '367JourneyDataExtract24Apr2023-30Apr2023.csv', '380JourneyDataExtract15Sep2023-30Sep2023.csv', '234JourneyDataExtract30Sep2020-06Oct2020.csv', '63JourneyDataExtract21Jun2017-27Jun2017.csv', '127JourneyDataExtract12Sep2018-18Sep2018.csv', '343JourneyDataExtract07Nov2022-13Nov2022.csv', '112JourneyDataExtract30May2018-05June2018.csv', '310JourneyDataExtract23Mar2022-29Mar2022.csv', '292JourneyDataExtract17Nov2021-23Nov2021.csv', '148JourneyDataExtract06Feb2019-12Feb2019.csv', '64JourneyDataExtract28Jun2017-04Jul2017.csv', '103JourneyDataExtract28Mar2018-03Apr2018.csv', '89JourneyDataExtract20Dec2017-26Dec2017.csv', '267JourneyDataExtract26May2021-01Jun2021.csv', '166JourneyDataExtract12Jun2019-18Jun2019.csv', '156JourneyDataExtract03Apr2019-09Apr2019.csv', '4a.JourneyDataExtract01Apr15-16Apr15.csv', '87JourneyDataExtract06Dec2017-12Dec2017.csv', '174JourneyDataExtract07Aug2019-13Aug2019.csv', '101JourneyDataExtract14Mar2018-20Mar2018.csv', '339JourneyDataExtract10Oct2022-16Oct2022.csv', '100JourneyDataExtract07Mar2018-13Mar2018.csv', '327JourneyDataExtract20Jul2022-26Jul2022.csv', '117JourneyDataExtract04July2018-10July2018.csv', '152JourneyDataExtract06Mar2019-12Mar2019.csv', '364JourneyDataExtract03Apr2023-09Apr2023.csv', '11JourneyDataExtract22Jun2016-28Jun2016.csv', '203JourneyDataExtract26Feb2020-03Mar2020.csv', '314JourneyDataExtract20Apr2022-26Apr2022.csv', '386JourneyDataExtract15Dec2023-31Dec2023.csv', '281JourneyDataExtract01Sep2021-07Sep2021.csv', '102JourneyDataExtract21Mar2018-27Mar2018.csv', '355JourneyDataExtract30Jan2023-05Feb2023.csv', '284JourneyDataExtract22Sep2021-28Sep2021.csv', '336JourneyDataExtract19Sep2022-25Sep2022.csv', '283JourneyDataExtract15Sep2021-21Sep2021.csv', '325JourneyDataExtract06Jul2022-12Jul2022.csv', '363JourneyDataExtract27Mar2023-02Apr2023.csv', '11a-Journey-Data-Extract-18Oct15-31Oct15.csv', '259JourneyDataExtract31Mar2021-06Apr2021.csv', '130JourneyDataExtract03Oct2018-09Oct2018.csv', '226JourneyDataExtract05Aug2020-11Aug2020.csv', '274JourneyDataExtract14Jul2021-20Jul2021.csv', '110JourneyDataExtract16May2018-22May2018.csv', '374JourneyDataExtract12Jun2023-18Jun2023.csv', '9a-Journey-Data-Extract-23Aug15-05Sep15.csv', '337JourneyDataExtract26Sep2022-02Oct2022.csv', '357JourneyDataExtract13Feb2023-19Feb2023.csv', '17JourneyDataExtract03Aug2016-09Aug2016.csv', '98JourneyDataExtract21Feb2018-27Feb2018.csv', '13aJourneyDataExtract13Dec15-24Dec15.csv', '13JourneyDataExtract06Jul2016-12Jul2016.csv', '326JourneyDataExtract13Jul2022-19Jul2022.csv', '272JourneyDataExtract30Jun2021-06Jul2021.csv', '57JourneyDataExtract10May2017-16May2017.csv', '290JourneyDataExtract03Nov2021-09Nov2021.csv', '43JourneyDataExtract01Feb2017-07Feb2017.csv', '241JourneyDataExtract18Nov2020-24Nov2020.csv', '269JourneyDataExtract09Jun2021-15Jun2021.csv', '79JourneyDataExtract11Oct2017-17Oct2017.csv', '80JourneyDataExtract18Oct2017-24Oct2017.csv', '157JourneyDataExtract10Apr2019-16Apr2019.csv', '168JourneyDataExtract26Jun2019-02Jul2019.csv', '189JourneyDataExtract20Nov2019-26Nov2019.csv', '99JourneyDataExtract28Feb2018-06Mar2018.csv', '378JourneyDataExtract15Aug2023-31Aug2023.csv', '177JourneyDataExtract28Aug2019-03Sep2019.csv', '366JourneyDataExtract17Apr2023-23Apr2023.csv', '318JourneyDataExtract18May2022-24May2022.csv', '187JourneyDataExtract06Nov2019-12Nov2019.csv', '15JourneyDataExtract20Jul2016-26Jul2016.csv', '142JourneyDataExtract26Dec2018-01Jan2019.csv', '109JourneyDataExtract09May2018-15May2018.csv', '128JourneyDataExtract19Sep2018-25Sep2018.csv', '213JourneyDataExtract06May2020-12May2020.csv', '23JourneyDataExtract14Sep2016-20Sep2016.csv', '24JourneyDataExtract21Sep2016-27Sep2016.csv', '347JourneyDataExtract05Dec2022-11Dec2022.csv', '01aJourneyDataExtract10Jan16-23Jan16.csv', '385JourneyDataExtract01Dec2023-14Dec2023.csv', '247JourneyDataExtract06Jan2021-12Jan2021.csv', '257JourneyDataExtract17Mar2021-23Mar2021.csv', '228JourneyDataExtract19Aug2020-25Aug2020.csv', '232JourneyDataExtract16Sep2020-22Sep2020.csv', '224JourneyDataExtract22Jul2020-28Jul2020.csv', '139JourneyDataExtract05Dec2018-11Dec2018.csv', '204JourneyDataExtract04Mar2020-10Mar2020.csv', '208JourneyDataExtract01Apr2020-07Apr2020.csv', '12bJourneyDataExtract28Nov15-12Dec15.csv', '270JourneyDataExtract16Jun2021-22Jun2021.csv', '311JourneyDataExtract30Mar2022-05Apr2022.csv', '81JourneyDataExtract25Oct2017-31Oct2017.csv', '47JourneyDataExtract01Mar2017-07Mar2017.csv', '231JourneyDataExtract09Sep2020-15Sep2020.csv', '372JourneyDataExtract29May2023-04Jun2023.csv', '362JourneyDataExtract20Mar2023-26Mar2023.csv', '242JourneyDataExtract25Nov2020-01Dec2020.csv', '54JourneyDataExtract19Apr2017-25Apr2017.csv', '59JourneyDataExtract24May2017-30May2017.csv', '235JourneyDataExtract07Oct2020-13Oct2020.csv', '225JourneyDataExtract29Jul2020-04Aug2020.csv', '13bJourneyDataExtract25Dec15-09Jan16.csv', '371JourneyDataExtract22May2023-28May2023.csv', '11b-Journey-Data-Extract-01Nov15-14Nov15.csv', '271JourneyDataExtract23Jun2021-29Jun2021.csv', '83JourneyDataExtract08Nov2017-14Nov2017.csv', '69JourneyDataExtract01Aug2017-07Aug2017.csv', '382JourneyDataExtract15Oct2023-31Oct2023.csv', '236JourneyDataExtract14Oct2020-20Oct2020.csv', '375JourneyDataExtract19Jun2023-30Jun2023.csv', '137JourneyDataExtract21Nov2018-27Nov2018.csv', '183JourneyDataExtract09Oct2019-15Oct2019.csv', '298JourneyDataExtract29Dec2021-04Jan2022.csv', '172JourneyDataExtract24Jul2019-30Jul2019.csv', '84JourneyDataExtract15Nov2017-21Nov2017.csv', '01bJourneyDataExtract24Jan16-06Feb16.csv', '393JourneyDataExtract01Apr2024-14Apr2024.csv', '182JourneyDataExtract02Oct2019-08Oct2019.csv', '368JourneyDataExtract01May2023-07May2023.csv', '77JourneyDataExtract27Sep2017-03Oct2017.csv', '185JourneyDataExtract23Oct2019-29Oct2019.csv', '123JourneyDataExtract15Aug2018-21Aug2018.csv', '97JourneyDataExtract14Feb2018-20Feb2018.csv', '92JourneyDataExtract10Jan2018-16Jan2018.csv', '359JourneyDataExtract27Feb2023-05Mar2023.csv', '291JourneyDataExtract10Nov2021-16Nov2021.csv', '124JourneyDataExtract22Aug2018-28Aug2018.csv', '256JourneyDataExtract10Mar2021-16Mar2021.csv', '45JourneyDataExtract15Feb2017-21Feb2017.csv', '195JourneyDataExtract01Jan2020-07Jan2020.csv', '96JourneyDataExtract07Feb2018-13Feb2018.csv', '118JourneyDataExtract11July2018-17July2018.csv', '212JourneyDataExtract29Apr2020-05May2020.csv', '244JourneyDataExtract09Dec2020-15Dec2020.csv', '136JourneyDataExtract14Nov2018-20Nov2018.csv', '260JourneyDataExtract07Apr2021-13Apr2021.csv', '167JourneyDataExtract19Jun2019-25Jun2019.csv', '246JourneyDataExtract23Dec2020-29Dec2020.csv', '246JourneyDataExtract30Dec2020-05Jan2021.csv', '322JourneyDataExtract15Jun2022-21Jun2022.csv', '72JourneyDataExtract23Aug2017-29Aug2017.csv', '28JourneyDataExtract19Oct2016-25Oct2016.csv', '58JourneyDataExtract17May2017-23May2017.csv', '394JourneyDataExtract15Apr2024-30Apr2024.csv', '107JourneyDataExtract25Apr2018-01May2018.csv', '02aJourneyDataExtract07Fe16-20Feb2016.csv', '5a.JourneyDataExtract03May15-16May15.csv', '76JourneyDataExtract20Sep2017-26Sep2017.csv', '170JourneyDataExtract10Jul2019-16Jul2019.csv', '3b.JourneyDataExtract16Mar15-31Mar15.csv', '165JourneyDataExtract05Jun2019-11Jun2019.csv', '114JourneyDataExtract13June2018-19June2018.csv', '40JourneyDataExtract11Jan2017-17Jan2017.csv', '161JourneyDataExtract08May2019-14May2019.csv', '390JourneyDataExtract15Feb2024-29Feb2024.csv', '36JourneyDataExtract14Dec2016-20Dec2016.csv', '171JourneyDataExtract17Jul2019-23Jul2019.csv', '10a-Journey-Data-Extract-20Sep15-03Oct15.csv', '38JourneyDataExtract28Dec2016-03Jan2017.csv', '153JourneyDataExtract13Mar2019-19Mar2019.csv', '243JourneyDataExtract02Dec2020-08Dec2020.csv', '151JourneyDataExtract27Feb2019-05Mar2019.csv', '132JourneyDataExtract17Oct2018-23Oct2018.csv', '67JourneyDataExtract19Jul2017-25Jul2017.csv', '29JourneyDataExtract26Oct2016-01Nov2016.csv', '188JourneyDataExtract13Nov2019-19Nov2019.csv', '150JourneyDataExtract20Feb2019-26Feb2019.csv', '46JourneyDataExtract22Feb2017-28Feb2017.csv', '328JourneyDataExtract27Jul2022-02Aug2022.csv', '378JourneyDataExtract01Aug2023-14Aug2023.csv', '392JourneyDataExtract15Mar2024-31Mar2024.csv', '206JourneyDataExtract18Mar2020-24Mar2020.csv', '202JourneyDataExtract19Feb2020-25Feb2020.csv', '95JourneyDataExtract31Jan2018-06Feb2018.csv', '71JourneyDataExtract15Aug2017-22Aug2017.csv', '266JourneyDataExtract19May2021-25May2021.csv', '254JourneyDataExtract24Feb2021-02Mar2021.csv'}\n"
     ]
    }
   ],
   "source": [
    "# use CSVs in dir\n",
    "# file_path = f\"data/tfl_CSVs/\"\n",
    "directory = f\"data/tfl_CSVs/\"\n",
    "downloaded_files = set(os.listdir(directory))\n",
    "CSVs = downloaded_files\n",
    "print(f\"There are {len(CSVs)} files: {CSVs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVs = CSVs[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Index: 396/417, merged_df is 6610.522654533386MB large:  95%|█████████▍| 396/417 [00:59<00:04,  4.31it/s] "
     ]
    }
   ],
   "source": [
    "# Initialize an empty Polars DataFrame\n",
    "\n",
    "CSVs_to_df = True\n",
    "if CSVs_to_df:\n",
    "    merged_df = pl.DataFrame()\n",
    "    cols = []\n",
    "\n",
    "    progress_bar = tqdm(CSVs, desc=\"Processing\")\n",
    "\n",
    "    for idx, csv in enumerate(progress_bar):\n",
    "        file_path = f\"{directory}{csv}\"\n",
    "        # print(f\"CSV: {csv}\\n file_path: {file_path}\")\n",
    "        df = pl.read_csv(file_path, skip_rows_after_header=1, ignore_errors=True)\n",
    "        # print(df.columns)\n",
    "\n",
    "        # Rename columns and keep only 'Rental ID', 'Start Date', 'Start Station Name', 'End Date', 'End Station Name'\n",
    "        renaming_dict = {\n",
    "            'Number': 'Rental ID',\n",
    "            'rental ID': 'Rental ID',\n",
    "            'Rental Id': 'Rental ID',\n",
    "            'Start date': 'Start Date',\n",
    "            'StartStation Name': 'Start Station Name',\n",
    "            'Start station': 'Start Station Name',\n",
    "            'End date': 'End Date',\n",
    "            'EndStation Name': 'End Station Name',\n",
    "            'End station': 'End Station Name'\n",
    "        }\n",
    "\n",
    "        for old_col, new_col in renaming_dict.items():\n",
    "            if old_col in df.columns:\n",
    "                df = df.rename({old_col: new_col})\n",
    "        \n",
    "        cols_to_keep = ['Rental ID', 'Start Date', 'Start Station Name', 'End Date', 'End Station Name']\n",
    "        df = df.select(cols_to_keep)\n",
    "\n",
    "        # Convert date columns\n",
    "        df = df.with_columns([\n",
    "            pl.col('Start Date').str.strptime(pl.Datetime, strict=False),\n",
    "            pl.col('End Date').str.strptime(pl.Datetime, strict=False),\n",
    "            pl.col('Start Station Name').cast(pl.Utf8),\n",
    "            pl.col('End Station Name').cast(pl.Utf8)\n",
    "        ])\n",
    "\n",
    "        # Filter rows (if needed)\n",
    "        # df = df.filter((pl.col(\"Start Station Name\") == \"Waterloo Station 3, Waterloo\") |\n",
    "        #                (pl.col(\"End Station Name\") == \"Waterloo Station 3, Waterloo\"))\n",
    "        \n",
    "        # Concatenate DataFrames\n",
    "        merged_df = pl.concat([merged_df, df])\n",
    "\n",
    "        progress_bar.set_description(\n",
    "            f\"Processing Index: {idx+1}/{len(CSVs)}, merged_df is {merged_df.estimated_size()/(1024*1024)}MB large\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pickle_dir = \"pickle/CSV_data_df\"\n",
    "if CSVs_to_df:\n",
    "    # save to pickle\n",
    "    with open(pickle_dir, 'wb') as f:\n",
    "        pickle.dump(merged_df, f)\n",
    "else:\n",
    "    #load from pickle\n",
    "    with open(pickle_dir, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (786, 2)\n",
      "┌─────┬─────────────────────────────────┐\n",
      "│ ID  ┆ Station Name                    │\n",
      "│ --- ┆ ---                             │\n",
      "│ u32 ┆ str                             │\n",
      "╞═════╪═════════════════════════════════╡\n",
      "│ 0   ┆ Abbey Orchard Street, Westmins… │\n",
      "│ 1   ┆ Abbotsbury Road, Holland Park   │\n",
      "│ 2   ┆ Aberdeen Place, St. John's Woo… │\n",
      "│ 3   ┆ Aberfeldy Street, Poplar        │\n",
      "│ 4   ┆ Abingdon Green, Westminster     │\n",
      "│ …   ┆ …                               │\n",
      "│ 781 ┆ Wormwood Street, Liverpool Str… │\n",
      "│ 782 ┆ Wren Street, Holborn            │\n",
      "│ 783 ┆ Wright's Lane, Kensington       │\n",
      "│ 784 ┆ Wynne Road, Stockwell           │\n",
      "│ 785 ┆ York Hall, Bethnal Green        │\n",
      "└─────┴─────────────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6504/363409236.py:9: DeprecationWarning: `DataFrame.with_row_count` is deprecated. Use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  id_mapping = unique_values.with_row_count(\"ID\")\n"
     ]
    }
   ],
   "source": [
    "start_stations = df.select(pl.col('Start Station Name').alias('Station Name'))\n",
    "end_stations = df.select(pl.col('End Station Name').alias('Station Name'))\n",
    "\n",
    "unique_values = (\n",
    "    pl.concat([start_stations, end_stations])\n",
    "    .unique()\n",
    "    .sort(\"Station Name\")\n",
    ")\n",
    "id_mapping = unique_values.with_row_count(\"ID\")\n",
    "print(id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Rental ID</th><th>Start Date</th><th>Start Station Name</th><th>End Date</th><th>End Station Name</th><th>Start_ID</th><th>End_ID</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>str</td><td>datetime[μs]</td><td>str</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>74656682</td><td>2018-04-18 07:32:00</td><td>&quot;Crosswall, Tower&quot;</td><td>2018-04-18 07:43:00</td><td>&quot;Tallis Street, Temple&quot;</td><td>182</td><td>684</td></tr><tr><td>74699122</td><td>2018-04-19 07:31:00</td><td>&quot;Crosswall, Tower&quot;</td><td>2018-04-19 07:42:00</td><td>&quot;Tallis Street, Temple&quot;</td><td>182</td><td>684</td></tr><tr><td>74831922</td><td>2018-04-21 22:42:00</td><td>&quot;Moor Street, Soho&quot;</td><td>2018-04-21 23:14:00</td><td>&quot;Geraldine Street, Elephant &amp; C…</td><td>452</td><td>268</td></tr><tr><td>74740907</td><td>2018-04-19 22:51:00</td><td>&quot;Moor Street, Soho&quot;</td><td>2018-04-19 23:10:00</td><td>&quot;Walworth Road, Elephant &amp; Cast…</td><td>452</td><td>731</td></tr><tr><td>74787286</td><td>2018-04-20 22:31:00</td><td>&quot;Moor Street, Soho&quot;</td><td>2018-04-20 22:46:00</td><td>&quot;Ontario Street, Elephant &amp; Cas…</td><td>452</td><td>496</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌───────────┬────────────────┬────────────────┬────────────────┬───────────────┬──────────┬────────┐\n",
       "│ Rental ID ┆ Start Date     ┆ Start Station  ┆ End Date       ┆ End Station   ┆ Start_ID ┆ End_ID │\n",
       "│ ---       ┆ ---            ┆ Name           ┆ ---            ┆ Name          ┆ ---      ┆ ---    │\n",
       "│ i64       ┆ datetime[μs]   ┆ ---            ┆ datetime[μs]   ┆ ---           ┆ u32      ┆ u32    │\n",
       "│           ┆                ┆ str            ┆                ┆ str           ┆          ┆        │\n",
       "╞═══════════╪════════════════╪════════════════╪════════════════╪═══════════════╪══════════╪════════╡\n",
       "│ 74656682  ┆ 2018-04-18     ┆ Crosswall,     ┆ 2018-04-18     ┆ Tallis        ┆ 182      ┆ 684    │\n",
       "│           ┆ 07:32:00       ┆ Tower          ┆ 07:43:00       ┆ Street,       ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Temple        ┆          ┆        │\n",
       "│ 74699122  ┆ 2018-04-19     ┆ Crosswall,     ┆ 2018-04-19     ┆ Tallis        ┆ 182      ┆ 684    │\n",
       "│           ┆ 07:31:00       ┆ Tower          ┆ 07:42:00       ┆ Street,       ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Temple        ┆          ┆        │\n",
       "│ 74831922  ┆ 2018-04-21     ┆ Moor Street,   ┆ 2018-04-21     ┆ Geraldine     ┆ 452      ┆ 268    │\n",
       "│           ┆ 22:42:00       ┆ Soho           ┆ 23:14:00       ┆ Street,       ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Elephant & C… ┆          ┆        │\n",
       "│ 74740907  ┆ 2018-04-19     ┆ Moor Street,   ┆ 2018-04-19     ┆ Walworth      ┆ 452      ┆ 731    │\n",
       "│           ┆ 22:51:00       ┆ Soho           ┆ 23:10:00       ┆ Road,         ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Elephant &    ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Cast…         ┆          ┆        │\n",
       "│ 74787286  ┆ 2018-04-20     ┆ Moor Street,   ┆ 2018-04-20     ┆ Ontario       ┆ 452      ┆ 496    │\n",
       "│           ┆ 22:31:00       ┆ Soho           ┆ 22:46:00       ┆ Street,       ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Elephant &    ┆          ┆        │\n",
       "│           ┆                ┆                ┆                ┆ Cas…          ┆          ┆        │\n",
       "└───────────┴────────────────┴────────────────┴────────────────┴───────────────┴──────────┴────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_ids = df.join(id_mapping.rename({\"Station Name\": \"Start Station Name\", \"ID\": \"Start_ID\"}), on=\"Start Station Name\", how=\"left\")\n",
    "df_with_ids = df_with_ids.join(id_mapping.rename({\"Station Name\": \"End Station Name\", \"ID\": \"End_ID\"}), on=\"End Station Name\", how=\"left\")\n",
    "\n",
    "df_with_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Rental ID</th><th>Start Date</th><th>End Date</th><th>Start_ID</th><th>End_ID</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>datetime[μs]</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>74656682</td><td>2018-04-18 07:32:00</td><td>2018-04-18 07:43:00</td><td>182</td><td>684</td></tr><tr><td>74699122</td><td>2018-04-19 07:31:00</td><td>2018-04-19 07:42:00</td><td>182</td><td>684</td></tr><tr><td>74831922</td><td>2018-04-21 22:42:00</td><td>2018-04-21 23:14:00</td><td>452</td><td>268</td></tr><tr><td>74740907</td><td>2018-04-19 22:51:00</td><td>2018-04-19 23:10:00</td><td>452</td><td>731</td></tr><tr><td>74787286</td><td>2018-04-20 22:31:00</td><td>2018-04-20 22:46:00</td><td>452</td><td>496</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌───────────┬─────────────────────┬─────────────────────┬──────────┬────────┐\n",
       "│ Rental ID ┆ Start Date          ┆ End Date            ┆ Start_ID ┆ End_ID │\n",
       "│ ---       ┆ ---                 ┆ ---                 ┆ ---      ┆ ---    │\n",
       "│ i64       ┆ datetime[μs]        ┆ datetime[μs]        ┆ u32      ┆ u32    │\n",
       "╞═══════════╪═════════════════════╪═════════════════════╪══════════╪════════╡\n",
       "│ 74656682  ┆ 2018-04-18 07:32:00 ┆ 2018-04-18 07:43:00 ┆ 182      ┆ 684    │\n",
       "│ 74699122  ┆ 2018-04-19 07:31:00 ┆ 2018-04-19 07:42:00 ┆ 182      ┆ 684    │\n",
       "│ 74831922  ┆ 2018-04-21 22:42:00 ┆ 2018-04-21 23:14:00 ┆ 452      ┆ 268    │\n",
       "│ 74740907  ┆ 2018-04-19 22:51:00 ┆ 2018-04-19 23:10:00 ┆ 452      ┆ 731    │\n",
       "│ 74787286  ┆ 2018-04-20 22:31:00 ┆ 2018-04-20 22:46:00 ┆ 452      ┆ 496    │\n",
       "└───────────┴─────────────────────┴─────────────────────┴──────────┴────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brief = df_with_ids[\"Rental ID\", \"Start Date\", \"End Date\", \"Start_ID\", \"End_ID\"]\n",
    "df_brief.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (50, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Rental ID</th><th>Date</th><th>ID</th><th>flow</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>u32</td><td>i32</td></tr></thead><tbody><tr><td>74654037</td><td>2018-04-18 00:00:00</td><td>551</td><td>-1</td></tr><tr><td>74654041</td><td>2018-04-18 00:00:00</td><td>204</td><td>-1</td></tr><tr><td>74654038</td><td>2018-04-18 00:00:00</td><td>552</td><td>-1</td></tr><tr><td>74654036</td><td>2018-04-18 00:00:00</td><td>670</td><td>-1</td></tr><tr><td>74654039</td><td>2018-04-18 00:00:00</td><td>695</td><td>-1</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>74654057</td><td>2018-04-18 00:10:00</td><td>587</td><td>1</td></tr><tr><td>74654066</td><td>2018-04-18 00:10:00</td><td>361</td><td>1</td></tr><tr><td>74654049</td><td>2018-04-18 00:10:00</td><td>83</td><td>1</td></tr><tr><td>74654075</td><td>2018-04-18 00:11:00</td><td>540</td><td>-1</td></tr><tr><td>74654076</td><td>2018-04-18 00:11:00</td><td>556</td><td>-1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (50, 4)\n",
       "┌───────────┬─────────────────────┬─────┬──────┐\n",
       "│ Rental ID ┆ Date                ┆ ID  ┆ flow │\n",
       "│ ---       ┆ ---                 ┆ --- ┆ ---  │\n",
       "│ i64       ┆ datetime[μs]        ┆ u32 ┆ i32  │\n",
       "╞═══════════╪═════════════════════╪═════╪══════╡\n",
       "│ 74654037  ┆ 2018-04-18 00:00:00 ┆ 551 ┆ -1   │\n",
       "│ 74654041  ┆ 2018-04-18 00:00:00 ┆ 204 ┆ -1   │\n",
       "│ 74654038  ┆ 2018-04-18 00:00:00 ┆ 552 ┆ -1   │\n",
       "│ 74654036  ┆ 2018-04-18 00:00:00 ┆ 670 ┆ -1   │\n",
       "│ 74654039  ┆ 2018-04-18 00:00:00 ┆ 695 ┆ -1   │\n",
       "│ …         ┆ …                   ┆ …   ┆ …    │\n",
       "│ 74654057  ┆ 2018-04-18 00:10:00 ┆ 587 ┆ 1    │\n",
       "│ 74654066  ┆ 2018-04-18 00:10:00 ┆ 361 ┆ 1    │\n",
       "│ 74654049  ┆ 2018-04-18 00:10:00 ┆ 83  ┆ 1    │\n",
       "│ 74654075  ┆ 2018-04-18 00:11:00 ┆ 540 ┆ -1   │\n",
       "│ 74654076  ┆ 2018-04-18 00:11:00 ┆ 556 ┆ -1   │\n",
       "└───────────┴─────────────────────┴─────┴──────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_start = df_brief[\"Rental ID\", \"Start Date\", \"Start_ID\"].rename({\"Start Date\": \"Date\", \"Start_ID\": \"ID\"}).with_columns(pl.lit(-1).alias(\"flow\"))\n",
    "\n",
    "df_end = df_brief[\"Rental ID\", \"End Date\", \"End_ID\"].rename({\"End Date\": \"Date\", \"End_ID\": \"ID\"}).with_columns(pl.lit(1).alias(\"flow\"))\n",
    "\n",
    "df_flows = pl.concat([df_start, df_end]).sort(\"Date\")\n",
    "df_flows.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3_930, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>Rental ID</th><th>Date</th><th>flow</th></tr><tr><td>u32</td><td>i64</td><td>datetime[μs]</td><td>i32</td></tr></thead><tbody><tr><td>304</td><td>74654047</td><td>2018-04-18 00:01:00</td><td>-1</td></tr><tr><td>304</td><td>74654403</td><td>2018-04-18 03:47:00</td><td>-1</td></tr><tr><td>304</td><td>74654707</td><td>2018-04-18 06:19:00</td><td>-1</td></tr><tr><td>304</td><td>74655873</td><td>2018-04-18 07:12:00</td><td>-1</td></tr><tr><td>304</td><td>74656789</td><td>2018-04-18 07:35:00</td><td>-1</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>512</td><td>74656298</td><td>2018-04-18 07:24:00</td><td>-1</td></tr><tr><td>512</td><td>74657379</td><td>2018-04-18 07:45:00</td><td>-1</td></tr><tr><td>512</td><td>74657476</td><td>2018-04-18 07:47:00</td><td>-1</td></tr><tr><td>512</td><td>74657646</td><td>2018-04-18 07:50:00</td><td>-1</td></tr><tr><td>512</td><td>74657835</td><td>2018-04-18 07:53:00</td><td>-1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3_930, 4)\n",
       "┌─────┬───────────┬─────────────────────┬──────┐\n",
       "│ ID  ┆ Rental ID ┆ Date                ┆ flow │\n",
       "│ --- ┆ ---       ┆ ---                 ┆ ---  │\n",
       "│ u32 ┆ i64       ┆ datetime[μs]        ┆ i32  │\n",
       "╞═════╪═══════════╪═════════════════════╪══════╡\n",
       "│ 304 ┆ 74654047  ┆ 2018-04-18 00:01:00 ┆ -1   │\n",
       "│ 304 ┆ 74654403  ┆ 2018-04-18 03:47:00 ┆ -1   │\n",
       "│ 304 ┆ 74654707  ┆ 2018-04-18 06:19:00 ┆ -1   │\n",
       "│ 304 ┆ 74655873  ┆ 2018-04-18 07:12:00 ┆ -1   │\n",
       "│ 304 ┆ 74656789  ┆ 2018-04-18 07:35:00 ┆ -1   │\n",
       "│ …   ┆ …         ┆ …                   ┆ …    │\n",
       "│ 512 ┆ 74656298  ┆ 2018-04-18 07:24:00 ┆ -1   │\n",
       "│ 512 ┆ 74657379  ┆ 2018-04-18 07:45:00 ┆ -1   │\n",
       "│ 512 ┆ 74657476  ┆ 2018-04-18 07:47:00 ┆ -1   │\n",
       "│ 512 ┆ 74657646  ┆ 2018-04-18 07:50:00 ┆ -1   │\n",
       "│ 512 ┆ 74657835  ┆ 2018-04-18 07:53:00 ┆ -1   │\n",
       "└─────┴───────────┴─────────────────────┴──────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flows_id = df_flows.group_by(\"ID\")\n",
    "df_flows_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>Rental ID</th><th>Date</th><th>ID</th><th>flow</th></tr><tr><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>551084.0</td><td>&quot;551084&quot;</td><td>551084.0</td><td>551084.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>7.4800e7</td><td>&quot;2018-04-21 10:54:29.198126&quot;</td><td>392.997998</td><td>0.0</td></tr><tr><td>&quot;std&quot;</td><td>84004.495487</td><td>null</td><td>234.76011</td><td>1.000001</td></tr><tr><td>&quot;min&quot;</td><td>7.4654033e7</td><td>&quot;2018-04-18 00:00:00&quot;</td><td>0.0</td><td>-1.0</td></tr><tr><td>&quot;25%&quot;</td><td>7.4726946e7</td><td>&quot;2018-04-19 18:01:00&quot;</td><td>189.0</td><td>-1.0</td></tr><tr><td>&quot;50%&quot;</td><td>7.4799651e7</td><td>&quot;2018-04-21 12:24:00&quot;</td><td>388.0</td><td>1.0</td></tr><tr><td>&quot;75%&quot;</td><td>7.4872807e7</td><td>&quot;2018-04-22 23:10:00&quot;</td><td>596.0</td><td>1.0</td></tr><tr><td>&quot;max&quot;</td><td>7.4944543e7</td><td>&quot;2018-04-24 23:59:00&quot;</td><td>785.0</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 5)\n",
       "┌────────────┬──────────────┬────────────────────────────┬────────────┬──────────┐\n",
       "│ statistic  ┆ Rental ID    ┆ Date                       ┆ ID         ┆ flow     │\n",
       "│ ---        ┆ ---          ┆ ---                        ┆ ---        ┆ ---      │\n",
       "│ str        ┆ f64          ┆ str                        ┆ f64        ┆ f64      │\n",
       "╞════════════╪══════════════╪════════════════════════════╪════════════╪══════════╡\n",
       "│ count      ┆ 551084.0     ┆ 551084                     ┆ 551084.0   ┆ 551084.0 │\n",
       "│ null_count ┆ 0.0          ┆ 0                          ┆ 0.0        ┆ 0.0      │\n",
       "│ mean       ┆ 7.4800e7     ┆ 2018-04-21 10:54:29.198126 ┆ 392.997998 ┆ 0.0      │\n",
       "│ std        ┆ 84004.495487 ┆ null                       ┆ 234.76011  ┆ 1.000001 │\n",
       "│ min        ┆ 7.4654033e7  ┆ 2018-04-18 00:00:00        ┆ 0.0        ┆ -1.0     │\n",
       "│ 25%        ┆ 7.4726946e7  ┆ 2018-04-19 18:01:00        ┆ 189.0      ┆ -1.0     │\n",
       "│ 50%        ┆ 7.4799651e7  ┆ 2018-04-21 12:24:00        ┆ 388.0      ┆ 1.0      │\n",
       "│ 75%        ┆ 7.4872807e7  ┆ 2018-04-22 23:10:00        ┆ 596.0      ┆ 1.0      │\n",
       "│ max        ┆ 7.4944543e7  ┆ 2018-04-24 23:59:00        ┆ 785.0      ┆ 1.0      │\n",
       "└────────────┴──────────────┴────────────────────────────┴────────────┴──────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flows.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5_475, 3)\n",
      "┌─────┬─────────────────────┬────────────┐\n",
      "│ ID  ┆ day                 ┆ total_flow │\n",
      "│ --- ┆ ---                 ┆ ---        │\n",
      "│ u32 ┆ datetime[μs]        ┆ i32        │\n",
      "╞═════╪═════════════════════╪════════════╡\n",
      "│ 161 ┆ 2018-04-24 00:00:00 ┆ -1         │\n",
      "│ 248 ┆ 2018-04-18 00:00:00 ┆ -3         │\n",
      "│ 125 ┆ 2018-04-19 00:00:00 ┆ 1          │\n",
      "│ 284 ┆ 2018-04-18 00:00:00 ┆ -14        │\n",
      "│ 78  ┆ 2018-04-19 00:00:00 ┆ -5         │\n",
      "│ …   ┆ …                   ┆ …          │\n",
      "│ 715 ┆ 2018-04-21 00:00:00 ┆ 6          │\n",
      "│ 109 ┆ 2018-04-21 00:00:00 ┆ 4          │\n",
      "│ 407 ┆ 2018-04-20 00:00:00 ┆ -5         │\n",
      "│ 659 ┆ 2018-04-19 00:00:00 ┆ 5          │\n",
      "│ 204 ┆ 2018-04-23 00:00:00 ┆ 0          │\n",
      "└─────┴─────────────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "df_summed_by_day = (\n",
    "    df_flows\n",
    "    .with_columns(pl.col(\"Date\").dt.truncate(\"1d\").alias(\"day\"))\n",
    "    .group_by([\"ID\", \"day\"])\n",
    "    .agg(pl.col(\"flow\").sum().alias(\"total_flow\"))\n",
    ")\n",
    "\n",
    "print(df_summed_by_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
